{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lingering problems:\n",
    "gpu problems\n",
    "* check raw hf (non sentence transformer mem usage forward backward), fp16, with lora and not with lora\n",
    "* check the effect of not loading the model to gpu (automatically) during the first load\n",
    "* MultipleNegativeLoss might influence gpu vram usage\n",
    "\n",
    "\n",
    "# data\n",
    "allminilm\n",
    "* gpu idle: 206MB\n",
    "* inference mode: 234MB\n",
    "* inference mode and autocast: 266MB\n",
    "\n",
    "alibaba\n",
    "* gpu idle: 1768MB\n",
    "* inference mode: 1818MB\n",
    "* inference mode and autocast: 2590MB\n",
    "* standard NO inference mode: 2100MB\n",
    "* lora NO inference mode: 2160MB\n",
    "\n",
    "alibaba backward:\n",
    "* vanilla: 3956\n",
    "* lora (r=8): 2182  (yes indeed lora helps!!)\n",
    "* lora (r=64): 2286  (yes indeed lora helps!!)\n",
    "\n",
    "alibaba backward, but using sentence transformer\n",
    "* vanilla: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/miniconda3/envs/kaggle_eedi/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import re\n",
    "from peft import LoraConfig, TaskType, get_peft_model  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_modules(model) -> list[str]:\n",
    "    if isinstance(model, BertModel):\n",
    "        return [\"query\", \"key\", \"value\", \"dense\"]\n",
    "    elif re.search(r\"Alibaba-NLP.+NewModel\", str(type(model))):\n",
    "        return [\"qkv_proj\", \"o_proj\", \"up_gate_proj\", \"down_proj\"]\n",
    "    raise ValueError(\n",
    "        f\"Model with type {type(model)} is unsupported, please manually inspect and add lora modules.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "\n",
    "\n",
    "st_model = SentenceTransformer(model_name, trust_remote_code=True)  # automatically moved to cuda here by sentence transformer\n",
    "# hf_model = AutoModel.from_pretrained(model_name, trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora\n",
    "lora_modules = get_target_modules(st_model[0]._modules[\"auto_model\"])\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    target_modules=lora_modules,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,  # just set to 2 * alpha as a rule of thumb\n",
    "    lora_dropout=0.2,\n",
    ")\n",
    "st_model[0]._modules[\"auto_model\"] = get_peft_model(\n",
    "    st_model[0]._modules[\"auto_model\"],\n",
    "    peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in lora_model.named_parameters():\n",
    "#     print(param.requires_grad, \"\\t\", param.device, \"\\t\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The dog (Canis familiaris or Canis lupus familiaris) is a domesticated descendant of the wolf. Also called the domestic dog, it was selectively bred from an extinct population of wolves during the Late Pleistocene by hunter-gatherers. The dog was the first species to be domesticated by humans, over 14,000 years ago and before the development of agriculture. Experts estimate that due to their long association with humans, dogs have gained the ability to thrive on a starch-rich diet that would be inadequate for other canids.\"\n",
    "\n",
    "enc = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "out = st_model(enc)\n",
    "out[\"token_embeddings\"].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  3899,  1006,  2064,  2483,  5220,  2483,  2030,  2064,\n",
       "          2483, 11320, 12207,  5220,  2483,  1007,  2003,  1037,  4968,  4383,\n",
       "         12608,  1997,  1996,  4702,  1012,  2036,  2170,  1996,  4968,  3899,\n",
       "          1010,  2009,  2001, 13228,  2135, 13680,  2013,  2019,  8548,  2313,\n",
       "          1997,  8588,  2076,  1996,  2397, 25080,  2011,  4477,  1011,  8587,\n",
       "          2545,  1012,  1996,  3899,  2001,  1996,  2034,  2427,  2000,  2022,\n",
       "          4968,  4383,  2011,  4286,  1010,  2058,  2403,  1010,  2199,  2086,\n",
       "          3283,  1998,  2077,  1996,  2458,  1997,  5237,  1012,  8519, 10197,\n",
       "          2008,  2349,  2000,  2037,  2146,  2523,  2007,  4286,  1010,  6077,\n",
       "          2031,  4227,  1996,  3754,  2000, 25220,  2006,  1037,  2732,  2818,\n",
       "          1011,  4138,  8738,  2008,  2052,  2022, 14710,  2005,  2060,  2064,\n",
       "          9821,  1012,   102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'token_embeddings': tensor([[[ 0.0189,  0.5279, -0.9400,  ...,  1.2576, -1.0254,  0.2723],\n",
       "         [-0.0202,  0.1192, -1.3852,  ...,  1.1576, -1.1413,  0.3268],\n",
       "         [ 0.3061,  0.5899, -1.1180,  ...,  0.9122, -0.8564, -0.0158],\n",
       "         ...,\n",
       "         [-0.1371,  0.9227, -0.8348,  ...,  1.4065, -0.9238, -0.1312],\n",
       "         [ 0.1083,  0.6111, -0.5685,  ...,  1.0989, -1.0156,  0.2471],\n",
       "         [ 0.1036,  0.6067, -0.5579,  ...,  1.1147, -1.0132,  0.2609]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), 'sentence_embedding': tensor([[ 0.0189,  0.5279, -0.9400,  ...,  1.2576, -1.0254,  0.2723]],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentenceTransformer.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceTransformer.forward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_eedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
