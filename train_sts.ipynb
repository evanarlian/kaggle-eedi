{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.losses import CoSENTLoss, MultipleNegativesRankingLoss\n",
    "from datasets import Dataset as HFDataset\n",
    "from itertools import islice\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from usearch.index import Index\n",
    "import string\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "model_ = AutoModel.from_pretrained(model_name, trust_remote_code=True).cuda()\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(model_name)\n",
    "# load data\n",
    "all_mis_texts = pd.read_csv(\"data/misconception_mapping.csv\")[\"MisconceptionName\"].tolist()\n",
    "df = pd.read_csv(\"data/eedi-paraphrased/train.csv\")\n",
    "df[\"QuestionComplete\"] = (\n",
    "    \"Subject: \"\n",
    "    + df[\"SubjectName\"]\n",
    "    + \". Construct: \"\n",
    "    + df[\"ConstructName\"]\n",
    "    + \". Question: \"\n",
    "    + df[\"QuestionText\"]\n",
    "    + \". Correct answer: \"\n",
    "    + df[\"CorrectText\"]\n",
    "    + \". Wrong answer: \"\n",
    "    + df[\"WrongText\"]\n",
    "    + \".\"\n",
    ")\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.7, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(df, groups=df[\"QuestionId\"]))\n",
    "df_train = df.iloc[train_idx]\n",
    "df_val = df.iloc[val_idx]\n",
    "df_val = df_val[~df_val[\"QuestionAiCreated\"] & ~df_val[\"MisconceptionAiCreated\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MisconceptionId</th>\n",
       "      <th>QuestionComplete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: Angles in Triangles. Construct: Find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: Angles in Triangles. Construct: Find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: Angles in Triangles. Construct: Find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: Angles in Triangles. Construct: Find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: Angles in Triangles. Construct: Find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15268</th>\n",
       "      <td>2586</td>\n",
       "      <td>Subject: Rearranging Formula and Equations. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15269</th>\n",
       "      <td>2586</td>\n",
       "      <td>Subject: Rearranging Formula and Equations. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15270</th>\n",
       "      <td>2586</td>\n",
       "      <td>Subject: Rearranging Formula and Equations. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15271</th>\n",
       "      <td>2586</td>\n",
       "      <td>Subject: Rearranging Formula and Equations. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15272</th>\n",
       "      <td>2586</td>\n",
       "      <td>Subject: Rearranging Formula and Equations. Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15273 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MisconceptionId                                   QuestionComplete\n",
       "0                    0  Subject: Angles in Triangles. Construct: Find ...\n",
       "1                    0  Subject: Angles in Triangles. Construct: Find ...\n",
       "2                    0  Subject: Angles in Triangles. Construct: Find ...\n",
       "3                    0  Subject: Angles in Triangles. Construct: Find ...\n",
       "4                    0  Subject: Angles in Triangles. Construct: Find ...\n",
       "...                ...                                                ...\n",
       "15268             2586  Subject: Rearranging Formula and Equations. Co...\n",
       "15269             2586  Subject: Rearranging Formula and Equations. Co...\n",
       "15270             2586  Subject: Rearranging Formula and Equations. Co...\n",
       "15271             2586  Subject: Rearranging Formula and Equations. Co...\n",
       "15272             2586  Subject: Rearranging Formula and Equations. Co...\n",
       "\n",
       "[15273 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q = (\n",
    "    df_train[[\"MisconceptionId\", \"QuestionComplete\"]]\n",
    "    .sort_values(\"MisconceptionId\")\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MisconceptionId</th>\n",
       "      <th>MisconceptionText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Unaware that the total of angles in a triangle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Lacks knowledge that the angles within a trian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Is not aware that the sum of angles in a trian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Doesn't understand that the angles inside a tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Does not know that angles in a triangle sum to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>2586</td>\n",
       "      <td>Misinterprets the rules governing the sequence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>2586</td>\n",
       "      <td>Does not correctly understand how to prioritiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>2586</td>\n",
       "      <td>Misunderstands order of operations in algebrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>2586</td>\n",
       "      <td>Fails to grasp the correct sequence for perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>2586</td>\n",
       "      <td>Is confused about the proper order of operatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8020 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MisconceptionId                                  MisconceptionText\n",
       "0                   0  Unaware that the total of angles in a triangle...\n",
       "1                   0  Lacks knowledge that the angles within a trian...\n",
       "2                   0  Is not aware that the sum of angles in a trian...\n",
       "3                   0  Doesn't understand that the angles inside a tr...\n",
       "4                   0  Does not know that angles in a triangle sum to...\n",
       "...               ...                                                ...\n",
       "8015             2586  Misinterprets the rules governing the sequence...\n",
       "8016             2586  Does not correctly understand how to prioritiz...\n",
       "8017             2586  Misunderstands order of operations in algebrai...\n",
       "8018             2586  Fails to grasp the correct sequence for perfor...\n",
       "8019             2586  Is confused about the proper order of operatio...\n",
       "\n",
       "[8020 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this is not a bug, we look for misconceptions from the full dataset, not train dataset\n",
    "df_m = (\n",
    "    temp[[\"MisconceptionId\", \"MisconceptionText\"]]\n",
    "    .sort_values(\"MisconceptionId\")\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def batched_inference(model, tokenizer, texts: list[str], bs: int, desc: str) -> Tensor:\n",
    "    \"\"\"Basically SentenceTransformer.encode, but consume less vram.\"\"\"\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), bs), desc=desc):\n",
    "        # max_length=256 comes from plotting the complete question text, and 256 covers 99%\n",
    "        encoded = tokenizer(\n",
    "            texts[i : i + bs],\n",
    "            max_length=256,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        outputs = model(**encoded)\n",
    "        emb = outputs.last_hidden_state[:, 0]  # cls token\n",
    "        emb = F.normalize(emb, p=2, dim=-1)\n",
    "        embeddings.append(emb.cpu())\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hn_mine_hf(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    q_texts: list[str],\n",
    "    q_mis_ids: list[int],\n",
    "    mis_texts: list[str],\n",
    "    mis_ids: list[int],\n",
    "    k: int,\n",
    "    bs: int,\n",
    ") -> list[list[int]]:\n",
    "    \"\"\"Hard negative mining, but different from: https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.mine_hard_negatives.\n",
    "    Sentence Transformers' version assumes different rows are always negatives, but that is not the case if we use paraphrased data.\n",
    "\n",
    "    Args:\n",
    "        q_texts (list[str]): Question texts.\n",
    "        q_mis_ids (list[int]): Ground truth misconception ids for the questions.\n",
    "        mis_texts (list[str]): Misconception texts.\n",
    "        mis_ids (list[int]): Misconception ids.\n",
    "        k (int): Top k hard misconception ids per question.\n",
    "        bs (int): Batch size.\n",
    "\n",
    "    Returns:\n",
    "        list[list[int]]:\n",
    "            Hard misconceptions for each question.\n",
    "            This is NOT misconception ids, but the actual list index.\n",
    "    \"\"\"\n",
    "    assert len(q_texts) == len(q_mis_ids)\n",
    "    assert len(mis_texts) == len(mis_ids)\n",
    "    m_embeds = batched_inference(\n",
    "        model, tokenizer, mis_texts, bs=bs, desc=\"miscon\"\n",
    "    ).numpy()\n",
    "    index = Index(ndim=m_embeds.shape[-1], metric=\"ip\")\n",
    "    index.add(np.arange(m_embeds.shape[0]), m_embeds)\n",
    "    q_embeds = batched_inference(\n",
    "        model, tokenizer, q_texts, bs=bs, desc=\"questions\"\n",
    "    ).numpy()\n",
    "    batch_matches = index.search(q_embeds, count=k)\n",
    "    hards = []\n",
    "    for i, matches in enumerate(batch_matches):  # type: ignore\n",
    "        nth_miscons: list[int] = [m.key for m in matches]\n",
    "        hard_miscons = [nth for nth in nth_miscons if mis_ids[nth] != q_mis_ids[i]]\n",
    "        hards.append(hard_miscons)\n",
    "    assert len(hards) == len(q_texts)\n",
    "    return hards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hn_mine_st(\n",
    "    model: SentenceTransformer,\n",
    "    q_texts: list[str],\n",
    "    q_mis_ids: list[int],\n",
    "    mis_texts: list[str],\n",
    "    mis_ids: list[int],\n",
    "    k: int,\n",
    "    bs: int,\n",
    ") -> list[list[int]]:\n",
    "    \"\"\"Hard negative mining, but different from: https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.mine_hard_negatives.\n",
    "    Sentence Transformers' version assumes different rows are always negatives, but that is not the case if we use paraphrased data.\n",
    "\n",
    "    Args:\n",
    "        q_texts (list[str]): Question texts.\n",
    "        q_mis_ids (list[int]): Ground truth misconception ids for the questions.\n",
    "        mis_texts (list[str]): Misconception texts.\n",
    "        mis_ids (list[int]): Misconception ids.\n",
    "        k (int): Top k hard misconception ids per question (at max).\n",
    "        bs (int): Batch size.\n",
    "\n",
    "    Returns:\n",
    "        list[list[int]]:\n",
    "            Hard misconceptions for each question.\n",
    "            This is NOT misconception ids, but the actual list index.\n",
    "    \"\"\"\n",
    "    assert len(q_texts) == len(q_mis_ids)\n",
    "    assert len(mis_texts) == len(mis_ids)\n",
    "    m_embeds = model.encode(\n",
    "        mis_texts,\n",
    "        batch_size=bs,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    index = Index(ndim=m_embeds.shape[-1], metric=\"ip\")\n",
    "    index.add(np.arange(m_embeds.shape[0]), m_embeds)\n",
    "    q_embeds = model.encode(\n",
    "        q_texts,\n",
    "        batch_size=bs,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    batch_matches = index.search(q_embeds, count=k)\n",
    "    hards = []\n",
    "    for i, matches in enumerate(batch_matches):  # type: ignore\n",
    "        nth_miscons: list[int] = [m.key for m in matches]\n",
    "        hard_miscons = [nth for nth in nth_miscons if mis_ids[nth] != q_mis_ids[i]]\n",
    "        hards.append(hard_miscons)\n",
    "    assert len(hards) == len(q_texts)\n",
    "    return hards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hards = hn_mine_hf(\n",
    "    model_,\n",
    "    tokenizer_,\n",
    "    q_texts=df_q[\"QuestionComplete\"].tolist(),\n",
    "    q_mis_ids=df_q[\"MisconceptionId\"].tolist(),\n",
    "    mis_texts=df_m[\"MisconceptionText\"].tolist(),\n",
    "    mis_ids=df_m[\"MisconceptionId\"].tolist(),\n",
    "    k=100,\n",
    "    bs=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952acf7a17af4026ac36ae2b46cca07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9f34d276c0497f9fe4cdbfa8638455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hards_st = hn_mine_st(\n",
    "    model,\n",
    "    q_texts=df_q[\"QuestionComplete\"].tolist(),\n",
    "    q_mis_ids=df_q[\"MisconceptionId\"].tolist(),\n",
    "    mis_texts=df_m[\"MisconceptionText\"].tolist(),\n",
    "    mis_ids=df_m[\"MisconceptionId\"].tolist(),\n",
    "    k=100,\n",
    "    bs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['q', 'mis', 'neg_1', 'neg_2', 'neg_3', 'neg_4', 'neg_5', 'neg_6', 'neg_7', 'neg_8', 'neg_9', 'neg_10'],\n",
       "    num_rows: 15293\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_mnr_dataset(\n",
    "    q_texts: list[str],\n",
    "    q_mis_ids: list[int],\n",
    "    mis_texts: list[str],\n",
    "    mis_ids: list[int],\n",
    "    hards: list[list[int]],\n",
    "    n_negatives: int,\n",
    ") -> HFDataset:\n",
    "    \"\"\"Create SentenceTransformer dataset suitable for MultipleNegativesRankingLoss.\n",
    "    The format is (anchor, positive, negative_1, â€¦, negative_n).\n",
    "    Example: https://huggingface.co/datasets/tomaarsen/gooaq-hard-negatives\n",
    "    \"\"\"\n",
    "    assert len(q_texts) == len(q_mis_ids) == len(hards)\n",
    "    assert len(mis_texts) == len(mis_ids)\n",
    "    assert all(n_negatives <= len(hard) for hard in hards)\n",
    "    # create reverse mapping\n",
    "    mis_id_to_mis_idx = defaultdict(list)\n",
    "    for i, mis_id in enumerate(mis_ids):\n",
    "        mis_id_to_mis_idx[mis_id].append(i)\n",
    "    # make hf dataset\n",
    "    d = {}\n",
    "    d[\"q\"], d[\"mis\"] = [], []\n",
    "    for i in range(1, n_negatives + 1):\n",
    "        d[f\"neg_{i}\"] = []\n",
    "    for i, (q_text, q_mis_id) in enumerate(zip(q_texts, q_mis_ids)):\n",
    "        rand_pos = random.choice(mis_id_to_mis_idx[q_mis_id])\n",
    "        rand_negs = random.sample(hards[i], k=n_negatives)\n",
    "        d[\"q\"].append(q_text)\n",
    "        d[\"mis\"].append(mis_texts[rand_pos])\n",
    "        for j, rand_neg in enumerate(rand_negs, 1):\n",
    "            d[f\"neg_{j}\"].append(mis_texts[rand_neg])\n",
    "    return HFDataset.from_dict(d)\n",
    "\n",
    "\n",
    "ds = make_mnr_dataset(\n",
    "    q_texts=df_q[\"QuestionComplete\"].tolist(),\n",
    "    q_mis_ids=df_q[\"MisconceptionId\"].tolist(),\n",
    "    mis_texts=df_m[\"MisconceptionText\"].tolist(),\n",
    "    mis_ids=df_m[\"MisconceptionId\"].tolist(),\n",
    "    hards=hards_st,\n",
    "    n_negatives=10,\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['q', 'mis', 'label'],\n",
       "    num_rows: 168223\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_cosent_dataset(\n",
    "    q_texts: list[str],\n",
    "    q_mis_ids: list[int],\n",
    "    mis_texts: list[str],\n",
    "    mis_ids: list[int],\n",
    "    hards: list[list[int]],\n",
    "    n_negatives: int,\n",
    ") -> HFDataset:\n",
    "    \"\"\"Create SentenceTransformer dataset suitable for CoSENTLoss.\n",
    "    The format is (sentence_A, sentence_B).\n",
    "    Example: https://sbert.net/docs/sentence_transformer/training_overview.html#loss-function\n",
    "    \"\"\"\n",
    "    assert len(q_texts) == len(q_mis_ids) == len(hards)\n",
    "    assert len(mis_texts) == len(mis_ids)\n",
    "    assert all(n_negatives <= len(hard) for hard in hards)\n",
    "    # create reverse mapping\n",
    "    mis_id_to_mis_idx = defaultdict(list)\n",
    "    for i, mis_id in enumerate(mis_ids):\n",
    "        mis_id_to_mis_idx[mis_id].append(i)\n",
    "    # make hf dataset\n",
    "    d = {\"q\": [], \"mis\": [], \"label\": []}\n",
    "    for i, (q_text, q_mis_id) in enumerate(zip(q_texts, q_mis_ids)):\n",
    "        # insert positive\n",
    "        rand_pos = random.choice(mis_id_to_mis_idx[q_mis_id])\n",
    "        d[\"q\"].append(q_text)\n",
    "        d[\"mis\"].append(mis_texts[rand_pos])\n",
    "        d[\"label\"].append(1.0)\n",
    "        # insert negatives\n",
    "        rand_negs = random.sample(hards[i], k=n_negatives)\n",
    "        for j, rand_neg in enumerate(rand_negs, 1):\n",
    "            d[\"q\"].append(q_text)\n",
    "            d[\"mis\"].append(mis_texts[rand_neg])\n",
    "            d[\"label\"].append(-1.0)\n",
    "    return HFDataset.from_dict(d)\n",
    "\n",
    "\n",
    "ds2 = make_cosent_dataset(\n",
    "    q_texts=df_q[\"QuestionComplete\"].tolist(),\n",
    "    q_mis_ids=df_q[\"MisconceptionId\"].tolist(),\n",
    "    mis_texts=df_m[\"MisconceptionText\"].tolist(),\n",
    "    mis_ids=df_m[\"MisconceptionId\"].tolist(),\n",
    "    hards=hards_st,\n",
    "    n_negatives=10,\n",
    ")\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw training, move this to script, and make cached hn mining because we will hammer the script a LOT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "# 1. Load a model to finetune with 2. (Optional) model card data\n",
    "# -- done upper\n",
    "\n",
    "# 3. Load a dataset to finetune on\n",
    "dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\")\n",
    "# -- done upper \n",
    "\n",
    "# 4. Define a loss function\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# 5. (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/gte-large-en-1\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"gte-large-en-1\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# 6. (Optional) Create an evaluator & evaluate the base model\n",
    "# dev_evaluator = TripletEvaluator(\n",
    "#     anchors=eval_dataset[\"anchor\"],\n",
    "#     positives=eval_dataset[\"positive\"],\n",
    "#     negatives=eval_dataset[\"negative\"],\n",
    "#     name=\"all-nli-dev\",\n",
    "# )\n",
    "# dev_evaluator(model)\n",
    "\n",
    "# 7. Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds2,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# (Optional) Evaluate the trained model on the test set\n",
    "test_evaluator = TripletEvaluator(\n",
    "    anchors=test_dataset[\"anchor\"],\n",
    "    positives=test_dataset[\"positive\"],\n",
    "    negatives=test_dataset[\"negative\"],\n",
    "    name=\"all-nli-test\",\n",
    ")\n",
    "test_evaluator(model)\n",
    "\n",
    "# 8. Save the trained model\n",
    "model.save_pretrained(\"models/mpnet-base-all-nli-triplet/final\")\n",
    "\n",
    "# 9. (Optional) Push it to the Hugging Face Hub\n",
    "model.push_to_hub(\"mpnet-base-all-nli-triplet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ef95dc8bb2401799b193a280c7cdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12d971b45d7401987373ebe913c1f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0001.parquet:   0%|          | 0.00/95.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03013e47f3c140929290c9dc79b056eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating corpus split:   0%|          | 0/382545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb0b1a5d817431688efc276b6c736d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "queries/queries/0000.parquet:   0%|          | 0.00/3.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f0e495d080449b97b02af6ac04a977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating queries split:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fdb538b9d34fc987aed42a4fc471ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c33001033734aeb86145e363eaf97b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/101k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14da0569c607434e82711a4bc004a179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de62a2c08594acebd4ca668369e2c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/382545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9cecca6bc748e6936b35f8938dbe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/382545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeIR-touche2020-subset-test_cosine_map@100\n",
      "0.30561077016600136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load a model\n",
    "modelmini = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the Touche-2020 IR dataset (https://huggingface.co/datasets/BeIR/webis-touche2020, https://huggingface.co/datasets/BeIR/webis-touche2020-qrels)\n",
    "corpus = load_dataset(\"BeIR/webis-touche2020\", \"corpus\", split=\"corpus\")\n",
    "queries = load_dataset(\"BeIR/webis-touche2020\", \"queries\", split=\"queries\")\n",
    "relevant_docs_data = load_dataset(\"BeIR/webis-touche2020-qrels\", split=\"test\")\n",
    "\n",
    "# For this dataset, we want to concatenate the title and texts for the corpus\n",
    "corpus = corpus.map(lambda x: {'text': x['title'] + \" \" + x['text']}, remove_columns=['title'])\n",
    "\n",
    "# Shrink the corpus size heavily to only the relevant documents + 30,000 random documents\n",
    "required_corpus_ids = set(map(str, relevant_docs_data[\"corpus-id\"]))\n",
    "required_corpus_ids |= set(random.sample(corpus[\"_id\"], k=30_000))\n",
    "corpus = corpus.filter(lambda x: x[\"_id\"] in required_corpus_ids)\n",
    "\n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(zip(corpus[\"_id\"], corpus[\"text\"]))  # Our corpus (cid => document)\n",
    "queries = dict(zip(queries[\"_id\"], queries[\"text\"]))  # Our queries (qid => question)\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for qid, corpus_ids in zip(relevant_docs_data[\"query-id\"], relevant_docs_data[\"corpus-id\"]):\n",
    "    qid = str(qid)\n",
    "    corpus_ids = str(corpus_ids)\n",
    "    if qid not in relevant_docs:\n",
    "        relevant_docs[qid] = set()\n",
    "    relevant_docs[qid].add(corpus_ids)\n",
    "\n",
    "# Given queries, a corpus and a mapping with relevant documents, the InformationRetrievalEvaluator computes different IR metrics.\n",
    "ir_evaluator = InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    name=\"BeIR-touche2020-subset-test\",\n",
    ")\n",
    "results = ir_evaluator(modelmini)\n",
    "'''\n",
    "Information Retrieval Evaluation of the model on the BeIR-touche2020-test dataset:\n",
    "Queries: 49\n",
    "Corpus: 31923\n",
    "\n",
    "Score-Function: cosine\n",
    "Accuracy@1: 77.55%\n",
    "Accuracy@3: 93.88%\n",
    "Accuracy@5: 97.96%\n",
    "Accuracy@10: 100.00%\n",
    "Precision@1: 77.55%\n",
    "Precision@3: 72.11%\n",
    "Precision@5: 71.43%\n",
    "Precision@10: 62.65%\n",
    "Recall@1: 1.72%\n",
    "Recall@3: 4.78%\n",
    "Recall@5: 7.90%\n",
    "Recall@10: 13.86%\n",
    "MRR@10: 0.8580\n",
    "NDCG@10: 0.6606\n",
    "MAP@100: 0.2934\n",
    "'''\n",
    "print(ir_evaluator.primary_metric)\n",
    "# => \"BeIR-touche2020-test_cosine_map@100\"\n",
    "print(results[ir_evaluator.primary_metric])\n",
    "# => 0.29335196224364596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ir_evaluator_dataset(\n",
    "    df: pd.DataFrame, all_mis_texts: list[str]\n",
    ") -> tuple[dict, dict, dict]:\n",
    "    temp = (\n",
    "        df[\n",
    "            [\n",
    "                \"QuestionId_Answer\",\n",
    "                \"QuestionComplete\",\n",
    "                \"MisconceptionId\",\n",
    "                \"MisconceptionText\",\n",
    "            ]\n",
    "        ]\n",
    "        .drop_duplicates()\n",
    "        .copy()\n",
    "    )\n",
    "    mapping = (\n",
    "        temp[[\"QuestionId_Answer\", \"MisconceptionId\"]]\n",
    "        .set_index(\"QuestionId_Answer\")[\"MisconceptionId\"]\n",
    "        .apply(lambda x: [x])\n",
    "        .to_dict()\n",
    "    )\n",
    "    q = (\n",
    "        temp[[\"QuestionId_Answer\", \"QuestionComplete\"]]\n",
    "        .set_index(\"QuestionId_Answer\")[\"QuestionComplete\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "    mis = {i: mis_text for i, mis_text in enumerate(all_mis_texts)}\n",
    "    return q, mis, mapping\n",
    "\n",
    "\n",
    "q, mis, mapping = make_ir_evaluator_dataset(df_val, all_mis_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO this shit is taking so much time lol\n",
    "# * the minconception must come from ALL dataset\n",
    "# * but the\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries=q,\n",
    "    corpus=mis,\n",
    "    relevant_docs=mapping,\n",
    "    map_at_k=[1, 3, 5, 10, 25],\n",
    "    batch_size=4,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cfe56d1b0a479fa5daec57ca629f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corpus Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator(modelmini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313, 2587)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q), len(mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cosine_accuracy@1': 0.09367859862909368,\n",
       " 'cosine_accuracy@3': 0.2063975628332064,\n",
       " 'cosine_accuracy@5': 0.2802741812642803,\n",
       " 'cosine_accuracy@10': 0.40594059405940597,\n",
       " 'cosine_precision@1': 0.09367859862909368,\n",
       " 'cosine_precision@3': 0.06879918761106879,\n",
       " 'cosine_precision@5': 0.05605483625285606,\n",
       " 'cosine_precision@10': 0.040594059405940595,\n",
       " 'cosine_recall@1': 0.09367859862909368,\n",
       " 'cosine_recall@3': 0.2063975628332064,\n",
       " 'cosine_recall@5': 0.2802741812642803,\n",
       " 'cosine_recall@10': 0.40594059405940597,\n",
       " 'cosine_ndcg@10': 0.22940932083812754,\n",
       " 'cosine_mrr@10': 0.17549112054062532,\n",
       " 'cosine_map@1': 0.09367859862909368,\n",
       " 'cosine_map@3': 0.1419141914191419,\n",
       " 'cosine_map@5': 0.15866971312515865,\n",
       " 'cosine_map@10': 0.1754911205406255,\n",
       " 'cosine_map@25': 0.18624516297058172,\n",
       " 'dot_accuracy@1': 0.09367859862909368,\n",
       " 'dot_accuracy@3': 0.2063975628332064,\n",
       " 'dot_accuracy@5': 0.2802741812642803,\n",
       " 'dot_accuracy@10': 0.40594059405940597,\n",
       " 'dot_precision@1': 0.09367859862909368,\n",
       " 'dot_precision@3': 0.06879918761106879,\n",
       " 'dot_precision@5': 0.05605483625285606,\n",
       " 'dot_precision@10': 0.040594059405940595,\n",
       " 'dot_recall@1': 0.09367859862909368,\n",
       " 'dot_recall@3': 0.2063975628332064,\n",
       " 'dot_recall@5': 0.2802741812642803,\n",
       " 'dot_recall@10': 0.40594059405940597,\n",
       " 'dot_ndcg@10': 0.22940932083812754,\n",
       " 'dot_mrr@10': 0.17549112054062532,\n",
       " 'dot_map@1': 0.09367859862909368,\n",
       " 'dot_map@3': 0.1419141914191419,\n",
       " 'dot_map@5': 0.15866971312515865,\n",
       " 'dot_map@10': 0.1754911205406255,\n",
       " 'dot_map@25': 0.18624516297058172}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_eedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
